{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9e8798-9bbb-44d1-aa1e-4444abfd77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6819b7c9-fa90-4700-b3e2-29a1cba95dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "    \n",
    "def q4():\n",
    "    t0 = time.time()\n",
    "\n",
    "    topic_name = 'test-topic'\n",
    "\n",
    "    for i in range(10):\n",
    "        message = {'number': i}\n",
    "        producer.send(topic_name, value=message)\n",
    "        print(f\"Sent: {message}\")\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f't1 took {(t1 - t0):.2f} seconds')\n",
    "\n",
    "    producer.flush()\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(f't2 took {(t2 - t1):.2f} seconds')\n",
    "    \n",
    "    \n",
    "def q5():\n",
    "    t0 = time.time()\n",
    "    \n",
    "    topic_name = 'green-trips'\n",
    "    df_green = pd.read_csv('green_tripdata_2019-10.csv.gz')\n",
    "    for row in df_green.itertuples(index=False):\n",
    "        row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "        # print(row_dict)\n",
    "        producer.send(topic_name, value=row_dict)\n",
    "        # break\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f't1 took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af62bb9-a14e-482a-9041-dd4031981717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: True\n"
     ]
    }
   ],
   "source": [
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "print('result:', producer.bootstrap_connected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9697b165-bb03-474c-ab3b-b8e885846c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,{kafka_jar_package} pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc45622-2d66-4b03-a600-37ee7fea0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec99590-78ef-4397-90de-df0e3e3f281f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "green_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0cc8d6-062e-4c0f-889d-56e52545bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1bba55-0744-48f2-a3e6-fae65438f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b2f6f6-46e9-4ce2-94e8-2a8913965b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lpep_pickup_datetime: string, lpep_dropoff_datetime: string, PULocationID: int, DOLocationID: int, passenger_count: double, trip_distance: double, tip_amount: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_stream"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c3269cb-1e36-4978-9406-0a018f23c39b",
   "metadata": {},
   "source": [
    "This is how you can do it:\n",
    "\n",
    "Add a column \"timestamp\" using the current_timestamp function\n",
    "Group by:\n",
    "5 minutes window based on the timestamp column (F.window(col(\"timestamp\"), \"5 minutes\"))\n",
    "\"DOLocationID\"\n",
    "Order by count\n",
    "You can print the output to the console using this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32630e39-f7f4-4afa-9fa4-a3cb52cf901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0461573d-d93a-4246-a3ea-426e20d62889",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3179215469.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    .orderBy(F.col('cnt').desc())\\\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "popular_destinations = green_stream\\\n",
    "    .withColumn('timestamp', F.current_timestamp())\\\n",
    "    .withColumn('window', F.window(F.col(\"timestamp\"), \"5 minutes\"))\\\n",
    "    .groupBy(F.col('window'), F.col('DOLocationID'))\\\n",
    "    .agg(F.count('lpep_pickup_datetime').alias('cnt'))\\\n",
    "    .orderBy(F.col('cnt').desc())\\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f7add0-e001-4422-abaf-4d08d6aef015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+---+\n",
      "|window|DOLocationID|cnt|\n",
      "+------+------------+---+\n",
      "+------+------------+---+\n",
      "\n",
      "+------+------------+---+\n",
      "|window|DOLocationID|cnt|\n",
      "+------+------------+---+\n",
      "+------+------------+---+\n",
      "\n",
      "+------+------------+---+\n",
      "|window|DOLocationID|cnt|\n",
      "+------+------------+---+\n",
      "+------+------------+---+\n",
      "\n",
      "+------+------------+---+\n",
      "|window|DOLocationID|cnt|\n",
      "+------+------------+---+\n",
      "+------+------------+---+\n",
      "\n",
      "+------+------------+---+\n",
      "|window|DOLocationID|cnt|\n",
      "+------+------------+---+\n",
      "+------+------------+---+\n",
      "\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|  cnt|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 00:40...|          74|17741|\n",
      "|{2024-04-09 00:40...|          42|15942|\n",
      "|{2024-04-09 00:40...|          41|14061|\n",
      "|{2024-04-09 00:40...|          75|12840|\n",
      "|{2024-04-09 00:40...|         129|11930|\n",
      "|{2024-04-09 00:40...|           7|11533|\n",
      "|{2024-04-09 00:40...|         166|10845|\n",
      "|{2024-04-09 00:40...|         236| 7913|\n",
      "|{2024-04-09 00:40...|         223| 7542|\n",
      "|{2024-04-09 00:40...|         238| 7318|\n",
      "+--------------------+------------+-----+\n",
      "\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|  cnt|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 00:40...|          74|17741|\n",
      "|{2024-04-09 00:40...|          42|15942|\n",
      "|{2024-04-09 00:40...|          41|14061|\n",
      "|{2024-04-09 00:40...|          75|12840|\n",
      "|{2024-04-09 00:40...|         129|11930|\n",
      "|{2024-04-09 00:40...|           7|11533|\n",
      "|{2024-04-09 00:40...|         166|10845|\n",
      "|{2024-04-09 00:40...|         236| 7913|\n",
      "|{2024-04-09 00:40...|         223| 7542|\n",
      "|{2024-04-09 00:40...|         238| 7318|\n",
      "+--------------------+------------+-----+\n",
      "\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|  cnt|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 00:40...|          74|17741|\n",
      "|{2024-04-09 00:40...|          42|15942|\n",
      "|{2024-04-09 00:40...|          41|14061|\n",
      "|{2024-04-09 00:40...|          75|12840|\n",
      "|{2024-04-09 00:40...|         129|11930|\n",
      "|{2024-04-09 00:40...|           7|11533|\n",
      "|{2024-04-09 00:40...|         166|10845|\n",
      "|{2024-04-09 00:40...|         236| 7913|\n",
      "|{2024-04-09 00:40...|         223| 7542|\n",
      "|{2024-04-09 00:40...|         238| 7318|\n",
      "+--------------------+------------+-----+\n",
      "\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|  cnt|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 00:40...|          74|17741|\n",
      "|{2024-04-09 00:40...|          42|15942|\n",
      "|{2024-04-09 00:40...|          41|14061|\n",
      "|{2024-04-09 00:40...|          75|12840|\n",
      "|{2024-04-09 00:40...|         129|11930|\n",
      "|{2024-04-09 00:40...|           7|11533|\n",
      "|{2024-04-09 00:40...|         166|10845|\n",
      "|{2024-04-09 00:40...|         236| 7913|\n",
      "|{2024-04-09 00:40...|         223| 7542|\n",
      "|{2024-04-09 00:40...|         238| 7318|\n",
      "+--------------------+------------+-----+\n",
      "\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|  cnt|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 00:40...|          74|17741|\n",
      "|{2024-04-09 00:40...|          42|15942|\n",
      "|{2024-04-09 00:40...|          41|14061|\n",
      "|{2024-04-09 00:40...|          75|12840|\n",
      "|{2024-04-09 00:40...|         129|11930|\n",
      "|{2024-04-09 00:40...|           7|11533|\n",
      "|{2024-04-09 00:40...|         166|10845|\n",
      "|{2024-04-09 00:40...|         236| 7913|\n",
      "|{2024-04-09 00:40...|         223| 7542|\n",
      "|{2024-04-09 00:40...|         238| 7318|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"green_view\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "# Use try-finally to ensure the streaming query is stopped properly when done\n",
    "try:\n",
    "    # Fetch and display the results periodically\n",
    "    for _ in range(10):  # Adjust the range for longer observation\n",
    "        spark.sql(\"SELECT * FROM green_view\").show()\n",
    "        time.sleep(10)  # Adjust the sleep time as needed\n",
    "finally:\n",
    "    query.stop()  # Stop the streaming query to release resources"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4daad9b6-9841-4d8c-95c6-0e5a33739d05",
   "metadata": {},
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e8d4b-b5fa-4099-83a4-2a76e3208264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
